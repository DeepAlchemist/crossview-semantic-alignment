# Total data nums:
# BigFACAD: 164555
# FashionGen: 60147
# Fashion200k: 76856
# PolyvoreOutfits: 71967
# Total: 373408

includes:
- configs/models/fashionvil/defaults.yaml

dataset_config:
  fashionall:
    data_dir: /mnt/csip-113/yangwenjie/dataset/
    attribute_label: True
    double_view: True
    processors:
        masked_image_processor:
          type: blockwise_masked_patch
        masked_token_processor:
          type: fashionvil_text_tokenizer
          params:
            tokenizer_config:
              #type: bert-base-uncased
              type: pretrained/bert-base-uncased/vocab.txt
              params:
                do_lower_case: true
                local_files_only: true
            mask_probability: 0.15
            max_seq_length: 75
            do_whole_word_mask: true

model_config:
  fashionvil:
    no_sharing: false  # sharing text encoder and fusion encoder
    image_encoder:
      type: torchvision_resnet
      params:
        name: resnet50
        pretrained: true
        zero_init_residual: false
        num_output_features: -1
        pool_type: avg
    image_tokenizer:
      type: vqvae_encoder
      params:
        pretrained_path: ${env.data_dir}/pretrained_models/vqvae_ema_pp_224_7x7_encoder_fashionall.pth
        resolution: 224
        num_tokens: 1024
        codebook_dim: 256
        attn_resolutions: [7]
        hidden_dim: 128
        in_channels: 3
        ch_mult: [1, 1, 2, 2, 4, 4]
        num_res_blocks: 2
        dropout: 0
        z_channels: 256
        double_z: false
    direct_features_input: false
    bypass_transformer: true
    bert_model_name: bert-base-uncased
    bert_model_dir: pretrained/bert-base-uncased
    training_head_type: pretraining
    task_for_inference: itc
    lr_multiplier: 20
    tasks:
      - itm
      - itc
    tasks_sample_ratio:
      - 0.4
      - 0.6

scheduler:
  type: multi_step
  params:
    use_warmup: true
    lr_steps:
    - 45000
    - 90000
    lr_ratio: 0.1
    warmup_iterations: 15000
    warmup_factor: 0.25

optimizer:
  type: adam_w
  params:
    lr: 1e-5
    eps: 1e-8
    weight_decay: 1e-4

evaluation:
  metrics:
  - type: r@k_general

training:
  experiment_name: fashionvil_e2e_pretrain_itc_itm
  batch_size: 4  # global batch size
  lr_scheduler: true
  max_updates: 120000
  log_interval: 10
  checkpoint_interval: 15000
  evaluation_interval: 15000
  find_unused_parameters: true
  early_stop:
    criteria: fashionall/r@k_general/avg
    minimize: false
  wandb:
    enabled: false 
    entity: hokkien-ywj
    project: multimodal-fashion

run_type: train_val
